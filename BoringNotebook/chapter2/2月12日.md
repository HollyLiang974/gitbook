# 2月12日

## 1. 学习内容

1. 在GitHub上找到一个Pytorch、Scikit-learn实现多种分类方法，包括逻辑回归（Logistic Regression）、多层感知机（MLP）、支持向量机（SVM）、K近邻（KNN）、CNN、RNN，极简代码适合新手小白入门，附英文实验报告（ACM模板）的[仓库](https://github.com/HollyLiang974/mnist-classification)

## 2. 问题

- [ ] 这些分类方法能否并行化？并行化的原理？PyTorch能否替代实现？

- [x] FATE上是否有、决策树、SVM、k-means[案例](https://github.com/FederatedAI/FATE/blob/master/examples/README.zh.md)？ 没有
  - 纵向:
    - LogisticRegression([benchmark_quality/hetero_lr](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/hetero_lr))
    - LinearRegression([benchmark_quality/hetero_linear_regression](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/hetero_linear_regression))
    - SecureBoost([benchmark_quality/hetero_sbt](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/hetero_sbt))
    - FastSecureBoost([benchmark_quality/hetero_fast_sbt](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/hetero_fast_sbt)),
    - NN([benchmark_quality/hetero_nn](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/hetero_nn))
  - 横向:
    - LogisticRegression([benchmark_quality/homo_lr](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/homo_lr))
    - SecureBoost([benchmark_quality/homo_sbt](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/homo_sbt))
    - NN([benchmark_quality/homo_nn](https://github.com/FederatedAI/FATE/blob/master/examples/benchmark_quality/homo_nn)

- [x] PyTorch相关的书上是否有案例？？  没有

   PyTorch是深度学习网络，支持线性回归，逻辑回归、卷积神经网络，循环神经网络，生成对抗网络

  [SVM需要自己改写](https://blog.csdn.net/qq_41335232/article/details/121040838)

- [x] PyTorch分布式都支持哪些网络呢？

  CNN、RNN、LSTM[^1]

- [x] sklearn如何分布式呢？

  sklearn 是 python 中一个非常著名的机器学习库，但是一般都是在单机上使用而不支持分布式计算，因此往往跟大规模的机器学习扯不上关系。这里通过 sklearn 进行的大规模机器学习指的也不是分布式机器学习，而是指**当数据量比内存要大时怎么通过 sklearn 进行机器学习，更准确来说是 out-of-core learning**， 这里涉及到的一个核心思想是**将数据转化为流式输入，然后通过 SGD 更新模型的参数**[^ 2]

  Scikit-learn使用[joblib](https://joblib.readthedocs.io/en/latest/)库在其估计器中支持并行计算。有关控制并行计算的开关，请参阅joblib文档。

  [sk-dist](https://github.com/Ibotta/sk-dist )是一个用于机器学习的Python模块，构建于scikit-learn之上，并在Apache 2.0软件许可下发布。sk-dist模块可以被认为是“分布式scikit-learn”，因为它的核心功能是将scikit-learn内置的joblib并行化的meta-estimator训练扩展到spark。

## [3. KNN可以实现并行化](https://blog.csdn.net/weixin_43622131/article/details/106966924)

### 3.1 KNN算法原理

#### [1. 欧式距离](https://zhuanlan.zhihu.com/p/520480277)：

**欧氏距离定义**： 欧氏距离（ Euclidean distance）是一个通常采用的距离定义，它是在m维空间中两个点之间的真实距离。

在二维和三维空间中的欧式距离的就是两点之间的距离，二维的公式是：

![img](https://pic2.zhimg.com/80/v2-a373464e87914d60946ee0ab3dc62b35_720w.webp)  
三维的公式是：

![img](https://pic1.zhimg.com/80/v2-fbc4eaf829fca50cab5a505f37be058c_720w.webp)  

推广到n维空间，欧式距离的公式是：

![img](https://pic3.zhimg.com/80/v2-90239e7d3045c05bda38f821b731bffe_720w.webp)

n维欧氏空间是一个点集，它的每个点可以表示为(x(1), x(2), …, x(n))，其中x(i)(i=1,2…n)是实数称为x的第i个坐标，两个点x和y之间的距离d(x, y)定义为上面的公式。

#### 2. KNN算法的基本思想即传统KNN算法的的性能瓶颈

KNN算法是一种很简单的分类算法，不需要构建模型，直接通过训练数据对测试数据进行分类，首先要定义一种度量样本之间距离的方法，对于鸢尾花数据集我所选用的是欧氏距离，只需要计算测试数据到所有训练数据的欧式距离，然后升序排列，取前N个数据的标签通过投票的方式来决定测试数据的样本，哪个类票数多就分为哪个类。
传统KNN算法在分析大数据的时候，当训练数据或者测试数据很大的时候，由于单机内存有限和单机计算资源有限，导致传统KNN算法失效，所以我们需要对其进行并行化实现。

#### 3. 并行化KNN

MapReduce的核心思想是分而治之，KNN之所以能够实现并行化是因为每个训练样本不受其他训练样本影响。因为训练数据量大所以将训练数据分布式存储读入map，因为测试数据量小所以将测试数据量作为全局文件读入，在map中每输入一个训练样本就计算它和所有测试数据的距离并传到reduce，然后reduce将同一个测试数据的距离合并然后排序计数得到标签，再输出。

### 3.2 MapReduce原理  

MapReduce是一种分布式数据处理技术，用于在大规模数据集上进行大量数据处理。它是一个高效的，简单易用的分布式系统，用于把大量的数据分成若干个小份，然后分布到多个节点上处理，最后再把所有的结果组合在一起。

MapReduce工作流程如下：

1. Map阶段：该阶段对每一份数据执行一个映射函数，将数据映射成（键，值）对的形式，即（Key, Value）。
2. Shuffle阶段：该阶段把所有的（键，值）对根据键进行分组，每个组都被分配到一个Reduce节点上。
3. Reduce阶段：该阶段对每个分组执行一个归纳函数，把所有值合并成一个输出。

MapReduce把数据处理任务分解成许多独立的任务，每个任务都是一个可以独立执行的任务。因此，MapReduce的效率很高，同时，由于任务的分解和分布，MapReduce也很容易扩展。

> [!NOTE|style:flat]
> KNN使用sklearn实现的，能否和PyTorch分布式包结合使用？



## 4. 数据并行计算的机器学习算法（ChatGPT）

以下是一些可以进行数据并行计算的机器学习算法：

1. 分布式随机森林（Distributed Random Forest）：该算法将数据分为多个部分并在分布式环境中并行计算，以加快模型训练速度。
2. 大规模支持向量机（Massive Scale Support Vector Machine）：该算法利用并行计算加速模型训练过程。
3. 分布式神经网络（Distributed Neural Network）：该算法通过分布式环境计算不同的神经元，以加快模型训练速度。
4. 分布式梯度下降（Distributed Gradient Descent）：该算法将数据分布在多个节点，通过并行计算减少训练时间。

这些算法可以使用工具如Hadoop、Spark等进行实现，以加快模型训练速度和提高计算效率



## 5. [分布式图神经网络](https://docs.dgl.ai/en/latest/guide_cn/distributed.html)（支持PyTorch）

## 总结

1. PyTorch分布式不支持决策树、KNN、SVM的训练，
2. 这些机器学习算法可能需要sklearn和spark
3. sk-dist可以应用哪些分布式场景中？
4. 数据并行的机器学习算法使用什么实现的，查一查名字
5. 高级的网络需要进一步的查资料



## 链接

[^1]: 舒娜,刘波,林伟伟,李鹏飞.分布式机器学习平台与算法综述[J].计算机科学,2019,46(03):9-18.
[^2]: https://wulc.me/2017/08/08/%E9%80%9A%E8%BF%87%20sklearn%20%E8%BF%9B%E8%A1%8C%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/

